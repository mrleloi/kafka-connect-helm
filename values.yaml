# Default values for kafka-connect.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: confluentinc/cp-kafka-connect
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

extraVolumeMounts:
#   []
   - name: plugin
     mountPath: /usr/share/confluent-hub-components
   - name: plugin2
     mountPath: /usr/share/lenses-components
extraVolumes:
#   []
   - name: plugin
     emptyDir: {}
   - name: plugin2
     emptyDir: {}

strategy:
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 0
  type: RollingUpdate

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
  automountServiceAccountToken: false

livenessProbe:
  httpGet:
    path: /
    port: connect
  initialDelaySeconds: 20
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

readinessProbe:
  httpGet:
    path: /
    port: connect
  initialDelaySeconds: 20
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

podAnnotations: {}

podSecurityContext:
  {}
  # fsGroup: 2000

securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: NodePort
  port: 8083

ingress:
  enabled: false
  className: "nginx"
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: "true"
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: kafka.baityapp.online
      paths:
        - path: /connect
          pathType: Prefix
  tls:
    - secretName: kafka.baityapp.online-tls
      hosts:
        - kafka.baityapp.online

resources:
  {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

configMapPairs:
  CONNECT_BOOTSTRAP_SERVERS: "kafka:9092"
  CONNECT_GROUP_ID: kafka-connect
  CONNECT_CONFIG_STORAGE_TOPIC: kafka-connect-config
  CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
  CONNECT_OFFSET_STORAGE_TOPIC: kafka-connect-offset
  CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
  CONNECT_OFFSET_STORAGE_PARTITIONS: "-1"
  CONNECT_OFFSET_PARTITION_NAME: kafka-connect.1
  CONNECT_STATUS_STORAGE_TOPIC: kafka-connect-status
  CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
  CONNECT_STATUS_STORAGE_PARTITIONS: "-1"
  CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
  CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
  CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://kafka-schema-registry:8081
  CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
  CONNECT_INTERNAL_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
  CONNECT_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
  CONNECT_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
  CONNECT_REST_ADVERTISED_HOST_NAME: connect
  CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/usr/share/lenses-components
  CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
  CONNECT_SECURITY_PROTOCOL: "SASL_PLAINTEXT"
  CONNECT_SASL_MECHANISM: "PLAIN"
  CONNECT_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="conduktor" password="123456a@A";'
  CONNECT_PRODUCER_SECURITY_PROTOCOL: "SASL_PLAINTEXT"
  CONNECT_PRODUCER_SASL_MECHANISM: "PLAIN"
  CONNECT_PRODUCER_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="conduktor" password="123456a@A";'
  CONNECT_CONSUMER_SECURITY_PROTOCOL: "SASL_PLAINTEXT"
  CONNECT_CONSUMER_SASL_MECHANISM: "PLAIN"
  CONNECT_CONSUMER_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="conduktor" password="123456a@A";'

initContainers:
#   []
   - name: init-plugin
     image: confluentinc/cp-kafka-connect:7.6.1 #7.2.2
     command: ["/bin/sh"]
     args: ["-c", "confluent-hub install confluentinc/kafka-connect-elasticsearch:14.0.14 --no-prompt && confluent-hub install confluentinc/kafka-connect-cassandra:2.0.6 --no-prompt"]
    #  command: ['/bin/sh', '-c',
    #   'confluent-hub install mongodb/kafka-connect-mongodb:1.11.2 --no-prompt && 
    #   confluent-hub install confluentinc/kafka-connect-elasticsearch:14.0.14 --no-prompt && 
    #   confluent-hub install confluentinc/kafka-connect-s3-source:2.5.17 --no-prompt && 
    #   confluent-hub install confluentinc/kafka-connect-http:1.7.5 --no-prompt && 
    #   confluent-hub install confluentinc/kafka-connect-prometheus-metrics:2.0.0 --no-prompt && 
    #   confluent-hub install confluentinc/kafka-connect-cassandra:2.0.6 --no-prompt && 
    #   confluent-hub install confluentinc/kafka-connect-jira:1.2.10 --no-prompt && 
    #   confluent-hub install dariobalinzo/kafka-connect-elasticsearch-source:1.5.5 --no-prompt; && 
    #   ]
    #  command:
    #   - sh
    #   - -c
     volumeMounts:
       - name: plugin
         mountPath: /usr/share/confluent-hub-components
#   []
   - name: init-plugin2
     image: alpine:latest
     command: ["/bin/sh"]
     args: ["-c", "apk add unzip wget && wget https://github.com/lensesio/stream-reactor/releases/download/6.3.0/kafka-connect-cassandra-6.3.0.zip && unzip kafka-connect-cassandra-6.3.0.zip && cp -f kafka-connect-cassandra-6.3.0/kafka-connect-cassandra-assembly-6.3.0.jar /usr/share/lenses-components/kafka-connect-cassandra-assembly-6.3.0.jar && chown -Rf 1000:1000 /usr/share/lenses-components"]
     volumeMounts:
       - name: plugin2
         mountPath: /usr/share/lenses-components

kafka:
  create: false
  fullnameOverride: kafka-connect-kafka
  nameOverride: kafka-connect-kafka
  defaultReplicationFactor: 1
  deleteTopicEnable: true
  heapOpts: "-Xmx1024m -Xms1024m"
  numPartitions: 1
  persistence:
    enabled: false
  provisioning:
    enabled: true
    topics:
      - name: kafka-connect-offset
        config:
          cleanup.policy: compact
      - name: kafka-connect-config
        config:
          cleanup.policy: compact
      - name: kafka-connect-status
        config:
          cleanup.policy: compact
  replicaCount: 1
  zookeeper:
    persistence:
      enabled: false

schema-registry:
  create: false
  externalKafka:
    brokers:
      - PLAINTEXT://kafka-broker-headless.bigdata.svc.cluster.local:9092
  ## Authentication parameters
  ## @param externalKafka.auth.protocol                   Authentication protocol. Allowed protocols: plaintext, tls, sasl and sasl_tls
  ## @param externalKafka.auth.jaas.user                  User for SASL authentication
  ## @param externalKafka.auth.jaas.password              Password for SASL authentication
  ##
  auth:
    ## Authentication protocol
    ## Supported values: 'plaintext', 'tls', sasl' and 'sasl_tls'
    ## This table shows the security provided on each protocol:
    ## | Method    | Authentication                | Encryption via TLS |
    ## | plaintext | None                          | No                 |
    ## | tls       | None                          | Yes                |
    ## | sasl      | Yes (via SASL)                | No                 |
    ## | sasl_tls  | Yes (via SASL)                | Yes                |
    ##
    protocol: sasl
    ## JAAS configuration for SASL authentication
    ## MANDATORY when protocol is 'sasl' or 'sasl_tls'
    ##
    jaas:
      user: "conduktor"
      password: "123456a@A"
  kafka:
    enabled: false
  zookeeper:
    enabled: false
